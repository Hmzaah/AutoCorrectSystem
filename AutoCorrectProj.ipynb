{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNK6VRRxaYdXMyWSOWtPKoX"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Xq8fq7Xgs9aq","executionInfo":{"status":"ok","timestamp":1726303528405,"user_tz":-330,"elapsed":62218,"user":{"displayName":"HAMZA KRISHNAGIRI (RA2211004010570)","userId":"05325100219040306490"}},"outputId":"9dff9c67-7fba-49a6-d496-27f80a64c3a1"},"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading collection 'all'\n","[nltk_data]    | \n","[nltk_data]    | Downloading package abc to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/abc.zip.\n","[nltk_data]    | Downloading package alpino to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/alpino.zip.\n","[nltk_data]    | Downloading package averaged_perceptron_tagger to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n","[nltk_data]    | Downloading package averaged_perceptron_tagger_eng to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping\n","[nltk_data]    |       taggers/averaged_perceptron_tagger_eng.zip.\n","[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping\n","[nltk_data]    |       taggers/averaged_perceptron_tagger_ru.zip.\n","[nltk_data]    | Downloading package averaged_perceptron_tagger_rus to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping\n","[nltk_data]    |       taggers/averaged_perceptron_tagger_rus.zip.\n","[nltk_data]    | Downloading package basque_grammars to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping grammars/basque_grammars.zip.\n","[nltk_data]    | Downloading package bcp47 to /root/nltk_data...\n","[nltk_data]    | Downloading package biocreative_ppi to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/biocreative_ppi.zip.\n","[nltk_data]    | Downloading package bllip_wsj_no_aux to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping models/bllip_wsj_no_aux.zip.\n","[nltk_data]    | Downloading package book_grammars to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping grammars/book_grammars.zip.\n","[nltk_data]    | Downloading package brown to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/brown.zip.\n","[nltk_data]    | Downloading package brown_tei to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/brown_tei.zip.\n","[nltk_data]    | Downloading package cess_cat to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/cess_cat.zip.\n","[nltk_data]    | Downloading package cess_esp to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/cess_esp.zip.\n","[nltk_data]    | Downloading package chat80 to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/chat80.zip.\n","[nltk_data]    | Downloading package city_database to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/city_database.zip.\n","[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/cmudict.zip.\n","[nltk_data]    | Downloading package comparative_sentences to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/comparative_sentences.zip.\n","[nltk_data]    | Downloading package comtrans to /root/nltk_data...\n","[nltk_data]    | Downloading package conll2000 to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/conll2000.zip.\n","[nltk_data]    | Downloading package conll2002 to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/conll2002.zip.\n","[nltk_data]    | Downloading package conll2007 to /root/nltk_data...\n","[nltk_data]    | Downloading package crubadan to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/crubadan.zip.\n","[nltk_data]    | Downloading package dependency_treebank to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/dependency_treebank.zip.\n","[nltk_data]    | Downloading package dolch to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/dolch.zip.\n","[nltk_data]    | Downloading package europarl_raw to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/europarl_raw.zip.\n","[nltk_data]    | Downloading package extended_omw to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    | Downloading package floresta to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/floresta.zip.\n","[nltk_data]    | Downloading package framenet_v15 to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/framenet_v15.zip.\n","[nltk_data]    | Downloading package framenet_v17 to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/framenet_v17.zip.\n","[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/gazetteers.zip.\n","[nltk_data]    | Downloading package genesis to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/genesis.zip.\n","[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n","[nltk_data]    | Downloading package ieer to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/ieer.zip.\n","[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/inaugural.zip.\n","[nltk_data]    | Downloading package indian to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/indian.zip.\n","[nltk_data]    | Downloading package jeita to /root/nltk_data...\n","[nltk_data]    | Downloading package kimmo to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/kimmo.zip.\n","[nltk_data]    | Downloading package knbc to /root/nltk_data...\n","[nltk_data]    | Downloading package large_grammars to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping grammars/large_grammars.zip.\n","[nltk_data]    | Downloading package lin_thesaurus to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/lin_thesaurus.zip.\n","[nltk_data]    | Downloading package mac_morpho to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/mac_morpho.zip.\n","[nltk_data]    | Downloading package machado to /root/nltk_data...\n","[nltk_data]    | Downloading package masc_tagged to /root/nltk_data...\n","[nltk_data]    | Downloading package maxent_ne_chunker to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n","[nltk_data]    | Downloading package maxent_ne_chunker_tab to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker_tab.zip.\n","[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping taggers/maxent_treebank_pos_tagger.zip.\n","[nltk_data]    | Downloading package maxent_treebank_pos_tagger_tab to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping\n","[nltk_data]    |       taggers/maxent_treebank_pos_tagger_tab.zip.\n","[nltk_data]    | Downloading package moses_sample to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping models/moses_sample.zip.\n","[nltk_data]    | Downloading package movie_reviews to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n","[nltk_data]    | Downloading package mte_teip5 to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/mte_teip5.zip.\n","[nltk_data]    | Downloading package mwa_ppdb to /root/nltk_data...\n","[nltk_data]    |   Unzipping misc/mwa_ppdb.zip.\n","[nltk_data]    | Downloading package names to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/names.zip.\n","[nltk_data]    | Downloading package nombank.1.0 to /root/nltk_data...\n","[nltk_data]    | Downloading package nonbreaking_prefixes to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/nonbreaking_prefixes.zip.\n","[nltk_data]    | Downloading package nps_chat to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/nps_chat.zip.\n","[nltk_data]    | Downloading package omw to /root/nltk_data...\n","[nltk_data]    | Downloading package omw-1.4 to /root/nltk_data...\n","[nltk_data]    | Downloading package opinion_lexicon to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/opinion_lexicon.zip.\n","[nltk_data]    | Downloading package panlex_swadesh to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    | Downloading package paradigms to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/paradigms.zip.\n","[nltk_data]    | Downloading package pe08 to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/pe08.zip.\n","[nltk_data]    | Downloading package perluniprops to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping misc/perluniprops.zip.\n","[nltk_data]    | Downloading package pil to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/pil.zip.\n","[nltk_data]    | Downloading package pl196x to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/pl196x.zip.\n","[nltk_data]    | Downloading package porter_test to /root/nltk_data...\n","[nltk_data]    |   Unzipping stemmers/porter_test.zip.\n","[nltk_data]    | Downloading package ppattach to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/ppattach.zip.\n","[nltk_data]    | Downloading package problem_reports to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/problem_reports.zip.\n","[nltk_data]    | Downloading package product_reviews_1 to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/product_reviews_1.zip.\n","[nltk_data]    | Downloading package product_reviews_2 to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/product_reviews_2.zip.\n","[nltk_data]    | Downloading package propbank to /root/nltk_data...\n","[nltk_data]    | Downloading package pros_cons to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/pros_cons.zip.\n","[nltk_data]    | Downloading package ptb to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/ptb.zip.\n","[nltk_data]    | Downloading package punkt to /root/nltk_data...\n","[nltk_data]    |   Unzipping tokenizers/punkt.zip.\n","[nltk_data]    | Downloading package punkt_tab to /root/nltk_data...\n","[nltk_data]    |   Unzipping tokenizers/punkt_tab.zip.\n","[nltk_data]    | Downloading package qc to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/qc.zip.\n","[nltk_data]    | Downloading package reuters to /root/nltk_data...\n","[nltk_data]    | Downloading package rslp to /root/nltk_data...\n","[nltk_data]    |   Unzipping stemmers/rslp.zip.\n","[nltk_data]    | Downloading package rte to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/rte.zip.\n","[nltk_data]    | Downloading package sample_grammars to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping grammars/sample_grammars.zip.\n","[nltk_data]    | Downloading package semcor to /root/nltk_data...\n","[nltk_data]    | Downloading package senseval to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/senseval.zip.\n","[nltk_data]    | Downloading package sentence_polarity to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/sentence_polarity.zip.\n","[nltk_data]    | Downloading package sentiwordnet to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/sentiwordnet.zip.\n","[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/shakespeare.zip.\n","[nltk_data]    | Downloading package sinica_treebank to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/sinica_treebank.zip.\n","[nltk_data]    | Downloading package smultron to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/smultron.zip.\n","[nltk_data]    | Downloading package snowball_data to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    | Downloading package spanish_grammars to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping grammars/spanish_grammars.zip.\n","[nltk_data]    | Downloading package state_union to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/state_union.zip.\n","[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/stopwords.zip.\n","[nltk_data]    | Downloading package subjectivity to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/subjectivity.zip.\n","[nltk_data]    | Downloading package swadesh to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/swadesh.zip.\n","[nltk_data]    | Downloading package switchboard to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/switchboard.zip.\n","[nltk_data]    | Downloading package tagsets to /root/nltk_data...\n","[nltk_data]    |   Unzipping help/tagsets.zip.\n","[nltk_data]    | Downloading package tagsets_json to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping help/tagsets_json.zip.\n","[nltk_data]    | Downloading package timit to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/timit.zip.\n","[nltk_data]    | Downloading package toolbox to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/toolbox.zip.\n","[nltk_data]    | Downloading package treebank to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/treebank.zip.\n","[nltk_data]    | Downloading package twitter_samples to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/twitter_samples.zip.\n","[nltk_data]    | Downloading package udhr to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/udhr.zip.\n","[nltk_data]    | Downloading package udhr2 to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/udhr2.zip.\n","[nltk_data]    | Downloading package unicode_samples to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/unicode_samples.zip.\n","[nltk_data]    | Downloading package universal_tagset to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping taggers/universal_tagset.zip.\n","[nltk_data]    | Downloading package universal_treebanks_v20 to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    | Downloading package vader_lexicon to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    | Downloading package verbnet to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/verbnet.zip.\n","[nltk_data]    | Downloading package verbnet3 to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/verbnet3.zip.\n","[nltk_data]    | Downloading package webtext to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/webtext.zip.\n","[nltk_data]    | Downloading package wmt15_eval to /root/nltk_data...\n","[nltk_data]    |   Unzipping models/wmt15_eval.zip.\n","[nltk_data]    | Downloading package word2vec_sample to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping models/word2vec_sample.zip.\n","[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n","[nltk_data]    | Downloading package wordnet2021 to /root/nltk_data...\n","[nltk_data]    | Downloading package wordnet2022 to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/wordnet2022.zip.\n","[nltk_data]    | Downloading package wordnet31 to /root/nltk_data...\n","[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n","[nltk_data]    | Downloading package words to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/words.zip.\n","[nltk_data]    | Downloading package ycoe to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/ycoe.zip.\n","[nltk_data]    | \n","[nltk_data]  Done downloading collection all\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":1}],"source":["import nltk\n","nltk.download('all')"]},{"cell_type":"code","source":["from google.colab import files\n","uploaded = files.upload()"],"metadata":{"id":"9Hj8e-Y4va6V","executionInfo":{"status":"ok","timestamp":1726469322066,"user_tz":-330,"elapsed":141304,"user":{"displayName":"HAMZA KRISHNAGIRI (RA2211004010570)","userId":"05325100219040306490"}},"colab":{"base_uri":"https://localhost:8080/","height":73},"outputId":"cf53534e-4cab-4cb9-9fe8-87c2a07d3b2d"},"execution_count":2,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","     <input type=\"file\" id=\"files-2bc72e88-6ff0-420d-8edd-28d3c5d1ffb4\" name=\"files[]\" multiple disabled\n","        style=\"border:none\" />\n","     <output id=\"result-2bc72e88-6ff0-420d-8edd-28d3c5d1ffb4\">\n","      Upload widget is only available when the cell has been executed in the\n","      current browser session. Please rerun this cell to enable.\n","      </output>\n","      <script>// Copyright 2017 Google LLC\n","//\n","// Licensed under the Apache License, Version 2.0 (the \"License\");\n","// you may not use this file except in compliance with the License.\n","// You may obtain a copy of the License at\n","//\n","//      http://www.apache.org/licenses/LICENSE-2.0\n","//\n","// Unless required by applicable law or agreed to in writing, software\n","// distributed under the License is distributed on an \"AS IS\" BASIS,\n","// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","// See the License for the specific language governing permissions and\n","// limitations under the License.\n","\n","/**\n"," * @fileoverview Helpers for google.colab Python module.\n"," */\n","(function(scope) {\n","function span(text, styleAttributes = {}) {\n","  const element = document.createElement('span');\n","  element.textContent = text;\n","  for (const key of Object.keys(styleAttributes)) {\n","    element.style[key] = styleAttributes[key];\n","  }\n","  return element;\n","}\n","\n","// Max number of bytes which will be uploaded at a time.\n","const MAX_PAYLOAD_SIZE = 100 * 1024;\n","\n","function _uploadFiles(inputId, outputId) {\n","  const steps = uploadFilesStep(inputId, outputId);\n","  const outputElement = document.getElementById(outputId);\n","  // Cache steps on the outputElement to make it available for the next call\n","  // to uploadFilesContinue from Python.\n","  outputElement.steps = steps;\n","\n","  return _uploadFilesContinue(outputId);\n","}\n","\n","// This is roughly an async generator (not supported in the browser yet),\n","// where there are multiple asynchronous steps and the Python side is going\n","// to poll for completion of each step.\n","// This uses a Promise to block the python side on completion of each step,\n","// then passes the result of the previous step as the input to the next step.\n","function _uploadFilesContinue(outputId) {\n","  const outputElement = document.getElementById(outputId);\n","  const steps = outputElement.steps;\n","\n","  const next = steps.next(outputElement.lastPromiseValue);\n","  return Promise.resolve(next.value.promise).then((value) => {\n","    // Cache the last promise value to make it available to the next\n","    // step of the generator.\n","    outputElement.lastPromiseValue = value;\n","    return next.value.response;\n","  });\n","}\n","\n","/**\n"," * Generator function which is called between each async step of the upload\n"," * process.\n"," * @param {string} inputId Element ID of the input file picker element.\n"," * @param {string} outputId Element ID of the output display.\n"," * @return {!Iterable<!Object>} Iterable of next steps.\n"," */\n","function* uploadFilesStep(inputId, outputId) {\n","  const inputElement = document.getElementById(inputId);\n","  inputElement.disabled = false;\n","\n","  const outputElement = document.getElementById(outputId);\n","  outputElement.innerHTML = '';\n","\n","  const pickedPromise = new Promise((resolve) => {\n","    inputElement.addEventListener('change', (e) => {\n","      resolve(e.target.files);\n","    });\n","  });\n","\n","  const cancel = document.createElement('button');\n","  inputElement.parentElement.appendChild(cancel);\n","  cancel.textContent = 'Cancel upload';\n","  const cancelPromise = new Promise((resolve) => {\n","    cancel.onclick = () => {\n","      resolve(null);\n","    };\n","  });\n","\n","  // Wait for the user to pick the files.\n","  const files = yield {\n","    promise: Promise.race([pickedPromise, cancelPromise]),\n","    response: {\n","      action: 'starting',\n","    }\n","  };\n","\n","  cancel.remove();\n","\n","  // Disable the input element since further picks are not allowed.\n","  inputElement.disabled = true;\n","\n","  if (!files) {\n","    return {\n","      response: {\n","        action: 'complete',\n","      }\n","    };\n","  }\n","\n","  for (const file of files) {\n","    const li = document.createElement('li');\n","    li.append(span(file.name, {fontWeight: 'bold'}));\n","    li.append(span(\n","        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n","        `last modified: ${\n","            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n","                                    'n/a'} - `));\n","    const percent = span('0% done');\n","    li.appendChild(percent);\n","\n","    outputElement.appendChild(li);\n","\n","    const fileDataPromise = new Promise((resolve) => {\n","      const reader = new FileReader();\n","      reader.onload = (e) => {\n","        resolve(e.target.result);\n","      };\n","      reader.readAsArrayBuffer(file);\n","    });\n","    // Wait for the data to be ready.\n","    let fileData = yield {\n","      promise: fileDataPromise,\n","      response: {\n","        action: 'continue',\n","      }\n","    };\n","\n","    // Use a chunked sending to avoid message size limits. See b/62115660.\n","    let position = 0;\n","    do {\n","      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n","      const chunk = new Uint8Array(fileData, position, length);\n","      position += length;\n","\n","      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n","      yield {\n","        response: {\n","          action: 'append',\n","          file: file.name,\n","          data: base64,\n","        },\n","      };\n","\n","      let percentDone = fileData.byteLength === 0 ?\n","          100 :\n","          Math.round((position / fileData.byteLength) * 100);\n","      percent.textContent = `${percentDone}% done`;\n","\n","    } while (position < fileData.byteLength);\n","  }\n","\n","  // All done.\n","  yield {\n","    response: {\n","      action: 'complete',\n","    }\n","  };\n","}\n","\n","scope.google = scope.google || {};\n","scope.google.colab = scope.google.colab || {};\n","scope.google.colab._files = {\n","  _uploadFiles,\n","  _uploadFilesContinue,\n","};\n","})(self);\n","</script> "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Saving final.txt to final.txt\n"]}]},{"cell_type":"code","source":["# importing regular expression\n","import re\n","\n","# words\n","w = []\n","\n","# reading text file\n","with open('final.txt', 'r', encoding=\"utf8\") as f:\n","    file_name_data = f.read()\n","    file_name_data = file_name_data.lower()\n","    w = re.findall('\\w+', file_name_data)\n","\n","# vocabulary\n","main_set = set(w)\n"],"metadata":{"id":"WTVt2Sn5wK8b","executionInfo":{"status":"ok","timestamp":1726469540443,"user_tz":-330,"elapsed":1299,"user":{"displayName":"HAMZA KRISHNAGIRI (RA2211004010570)","userId":"05325100219040306490"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["# Functions to count the frequency\n","# of the words in the whole text file\n","\n","\n","def counting_words(words):\n","    word_count = {}\n","    for word in words:\n","        if word in word_count:\n","            word_count[word] += 1\n","        else:\n","            word_count[word] = 1\n","    return word_count\n"],"metadata":{"id":"ONvhkP1DwNwj","executionInfo":{"status":"ok","timestamp":1726469503069,"user_tz":-330,"elapsed":421,"user":{"displayName":"HAMZA KRISHNAGIRI (RA2211004010570)","userId":"05325100219040306490"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["# Calculating the probability of each word\n","def prob_cal(word_count_dict):\n","    probs = {}\n","    m = sum(word_count_dict.values())\n","    for key in word_count_dict.keys():\n","        probs[key] = word_count_dict[key] / m\n","    return probs\n"],"metadata":{"id":"vqkKQDHAwR32","executionInfo":{"status":"ok","timestamp":1726469529379,"user_tz":-330,"elapsed":684,"user":{"displayName":"HAMZA KRISHNAGIRI (RA2211004010570)","userId":"05325100219040306490"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["pip install pattern"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"J2i6HkqMwaMZ","executionInfo":{"status":"ok","timestamp":1726469389829,"user_tz":-330,"elapsed":27913,"user":{"displayName":"HAMZA KRISHNAGIRI (RA2211004010570)","userId":"05325100219040306490"}},"outputId":"1dc4db86-9834-4838-bc6d-f408f3d0e419"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting pattern\n","  Downloading Pattern-3.6.0.tar.gz (22.2 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m22.2/22.2 MB\u001b[0m \u001b[31m43.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: future in /usr/local/lib/python3.10/dist-packages (from pattern) (1.0.0)\n","Collecting backports.csv (from pattern)\n","  Downloading backports.csv-1.0.7-py2.py3-none-any.whl.metadata (4.0 kB)\n","Collecting mysqlclient (from pattern)\n","  Downloading mysqlclient-2.2.4.tar.gz (90 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.4/90.4 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from pattern) (4.12.3)\n","Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from pattern) (4.9.4)\n","Collecting feedparser (from pattern)\n","  Downloading feedparser-6.0.11-py3-none-any.whl.metadata (2.4 kB)\n","Collecting pdfminer.six (from pattern)\n","  Downloading pdfminer.six-20240706-py3-none-any.whl.metadata (4.1 kB)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from pattern) (1.26.4)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from pattern) (1.13.1)\n","Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from pattern) (3.8.1)\n","Collecting python-docx (from pattern)\n","  Downloading python_docx-1.1.2-py3-none-any.whl.metadata (2.0 kB)\n","Collecting cherrypy (from pattern)\n","  Downloading CherryPy-18.10.0-py3-none-any.whl.metadata (8.7 kB)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from pattern) (2.32.3)\n","Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->pattern) (2.6)\n","Collecting cheroot>=8.2.1 (from cherrypy->pattern)\n","  Downloading cheroot-10.0.1-py3-none-any.whl.metadata (7.1 kB)\n","Collecting portend>=2.1.1 (from cherrypy->pattern)\n","  Downloading portend-3.2.0-py3-none-any.whl.metadata (3.6 kB)\n","Requirement already satisfied: more-itertools in /usr/local/lib/python3.10/dist-packages (from cherrypy->pattern) (10.3.0)\n","Collecting zc.lockfile (from cherrypy->pattern)\n","  Downloading zc.lockfile-3.0.post1-py3-none-any.whl.metadata (6.2 kB)\n","Collecting jaraco.collections (from cherrypy->pattern)\n","  Downloading jaraco.collections-5.1.0-py3-none-any.whl.metadata (3.9 kB)\n","Collecting sgmllib3k (from feedparser->pattern)\n","  Downloading sgmllib3k-1.0.0.tar.gz (5.8 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->pattern) (8.1.7)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->pattern) (1.4.2)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->pattern) (2024.5.15)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk->pattern) (4.66.5)\n","Requirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six->pattern) (3.3.2)\n","Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six->pattern) (43.0.1)\n","Requirement already satisfied: typing-extensions>=4.9.0 in /usr/local/lib/python3.10/dist-packages (from python-docx->pattern) (4.12.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->pattern) (3.8)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->pattern) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->pattern) (2024.8.30)\n","Collecting jaraco.functools (from cheroot>=8.2.1->cherrypy->pattern)\n","  Downloading jaraco.functools-4.0.2-py3-none-any.whl.metadata (2.8 kB)\n","Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=36.0.0->pdfminer.six->pattern) (1.17.1)\n","Collecting tempora>=1.8 (from portend>=2.1.1->cherrypy->pattern)\n","  Downloading tempora-5.7.0-py3-none-any.whl.metadata (3.2 kB)\n","Collecting jaraco.text (from jaraco.collections->cherrypy->pattern)\n","  Downloading jaraco.text-4.0.0-py3-none-any.whl.metadata (3.7 kB)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from zc.lockfile->cherrypy->pattern) (71.0.4)\n","Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six->pattern) (2.22)\n","Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from tempora>=1.8->portend>=2.1.1->cherrypy->pattern) (2.8.2)\n","Collecting jaraco.context>=4.1 (from jaraco.text->jaraco.collections->cherrypy->pattern)\n","  Downloading jaraco.context-6.0.1-py3-none-any.whl.metadata (4.1 kB)\n","Collecting autocommand (from jaraco.text->jaraco.collections->cherrypy->pattern)\n","  Downloading autocommand-2.2.2-py3-none-any.whl.metadata (15 kB)\n","Collecting backports.tarfile (from jaraco.context>=4.1->jaraco.text->jaraco.collections->cherrypy->pattern)\n","  Downloading backports.tarfile-1.2.0-py3-none-any.whl.metadata (2.0 kB)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil->tempora>=1.8->portend>=2.1.1->cherrypy->pattern) (1.16.0)\n","Downloading backports.csv-1.0.7-py2.py3-none-any.whl (12 kB)\n","Downloading CherryPy-18.10.0-py3-none-any.whl (349 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m349.8/349.8 kB\u001b[0m \u001b[31m23.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading feedparser-6.0.11-py3-none-any.whl (81 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.3/81.3 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading pdfminer.six-20240706-py3-none-any.whl (5.6 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m96.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading python_docx-1.1.2-py3-none-any.whl (244 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m244.3/244.3 kB\u001b[0m \u001b[31m19.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading cheroot-10.0.1-py3-none-any.whl (104 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.8/104.8 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading portend-3.2.0-py3-none-any.whl (5.3 kB)\n","Downloading jaraco.collections-5.1.0-py3-none-any.whl (11 kB)\n","Downloading zc.lockfile-3.0.post1-py3-none-any.whl (9.8 kB)\n","Downloading tempora-5.7.0-py3-none-any.whl (13 kB)\n","Downloading jaraco.functools-4.0.2-py3-none-any.whl (9.9 kB)\n","Downloading jaraco.text-4.0.0-py3-none-any.whl (11 kB)\n","Downloading jaraco.context-6.0.1-py3-none-any.whl (6.8 kB)\n","Downloading autocommand-2.2.2-py3-none-any.whl (19 kB)\n","Downloading backports.tarfile-1.2.0-py3-none-any.whl (30 kB)\n","Building wheels for collected packages: pattern, mysqlclient, sgmllib3k\n","  Building wheel for pattern (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pattern: filename=Pattern-3.6-py3-none-any.whl size=22332702 sha256=6ca516e9d42483040d707944d0de83c065be85a4a01615f2961d7e27d7760d1a\n","  Stored in directory: /root/.cache/pip/wheels/d1/8f/40/fe23abd593ef60be5bfaf3e02154d3484df42aa947bbf4d499\n","  Building wheel for mysqlclient (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for mysqlclient: filename=mysqlclient-2.2.4-cp310-cp310-linux_x86_64.whl size=124740 sha256=af61edb008ec78b8dc1cc81cdc5a86d16b4685975b1e589be3c218b84839aaa8\n","  Stored in directory: /root/.cache/pip/wheels/ac/96/ac/2a4d8cb58a4d95de1dffc3f8b0ea42e0e5b63ab97640edbda3\n","  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-py3-none-any.whl size=6047 sha256=1aab638b9435f5fdb440b36aa1c9a0cf627e0f75c6fff5cde12ac44465653b31\n","  Stored in directory: /root/.cache/pip/wheels/f0/69/93/a47e9d621be168e9e33c7ce60524393c0b92ae83cf6c6e89c5\n","Successfully built pattern mysqlclient sgmllib3k\n","Installing collected packages: sgmllib3k, backports.csv, zc.lockfile, python-docx, mysqlclient, jaraco.functools, feedparser, backports.tarfile, autocommand, tempora, jaraco.context, cheroot, portend, pdfminer.six, jaraco.text, jaraco.collections, cherrypy, pattern\n","Successfully installed autocommand-2.2.2 backports.csv-1.0.7 backports.tarfile-1.2.0 cheroot-10.0.1 cherrypy-18.10.0 feedparser-6.0.11 jaraco.collections-5.1.0 jaraco.context-6.0.1 jaraco.functools-4.0.2 jaraco.text-4.0.0 mysqlclient-2.2.4 pattern-3.6 pdfminer.six-20240706 portend-3.2.0 python-docx-1.1.2 sgmllib3k-1.0.0 tempora-5.7.0 zc.lockfile-3.0.post1\n"]},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["jaraco"]},"id":"a3ba8fd72055453daed1c253a9648ae4"}},"metadata":{}}]},{"cell_type":"code","source":["# LemmWord: extracting and adding\n","# root word i.e.Lemma using pattern module\n","import pattern\n","from pattern.en import lemma, lexeme\n","from nltk.stem import WordNetLemmatizer\n","\n","\n","def LemmWord(word):\n","    return list(lexeme(wd) for wd in word.split())[0]\n"],"metadata":{"id":"mABtQ-N_w288","executionInfo":{"status":"ok","timestamp":1726469535263,"user_tz":-330,"elapsed":494,"user":{"displayName":"HAMZA KRISHNAGIRI (RA2211004010570)","userId":"05325100219040306490"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["# Deleting letters from the words\n","def DeleteLetter(word):\n","    delete_list = []\n","    split_list = []\n","\n","    # considering letters 0 to i then i to -1\n","    # Leaving the ith letter\n","    for i in range(len(word)):\n","        split_list.append((word[0:i], word[i:]))\n","\n","    for a, b in split_list:\n","        delete_list.append(a + b[1:])\n","    return delete_list\n"],"metadata":{"id":"L7wdm1asw_Ub","executionInfo":{"status":"ok","timestamp":1726469425917,"user_tz":-330,"elapsed":504,"user":{"displayName":"HAMZA KRISHNAGIRI (RA2211004010570)","userId":"05325100219040306490"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["# Switching two letters in a word\n","def Switch_(word):\n","    split_list = []\n","    switch_l = []\n","\n","    #creating pair of the words(and breaking them)\n","    for i in range(len(word)):\n","        split_list.append((word[0:i], word[i:]))\n","\n","    #Printint the first word (i.e. a)\n","    #then replacing the first and second character of b\n","    switch_l = [a + b[1] + b[0] + b[2:] for a, b in split_list if len(b) >= 2]\n","    return switch_l\n"],"metadata":{"id":"ciZYoyGAxCMV","executionInfo":{"status":"ok","timestamp":1726469429264,"user_tz":-330,"elapsed":462,"user":{"displayName":"HAMZA KRISHNAGIRI (RA2211004010570)","userId":"05325100219040306490"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["def Replace_(word):\n","    split_l = []\n","    replace_list = []\n","\n","    # Replacing the letter one-by-one from the list of alphs\n","    for i in range(len(word)):\n","        split_l.append((word[0:i], word[i:]))\n","    alphs = 'abcdefghijklmnopqrstuvwxyz'\n","    replace_list = [a + l + (b[1:] if len(b) > 1 else '')\n","                    for a, b in split_l if b for l in alphs]\n","    return replace_list\n"],"metadata":{"id":"AzEygzlo2E_i","executionInfo":{"status":"ok","timestamp":1726469432836,"user_tz":-330,"elapsed":6,"user":{"displayName":"HAMZA KRISHNAGIRI (RA2211004010570)","userId":"05325100219040306490"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["def insert_(word):\n","    split_l = []\n","    insert_list = []\n","\n","    # Making pairs of the split words\n","    for i in range(len(word) + 1):\n","        split_l.append((word[0:i], word[i:]))\n","\n","    # Storing new words in a list\n","    # But one new character at each location\n","    alphs = 'abcdefghijklmnopqrstuvwxyz'\n","    insert_list = [a + l + b for a, b in split_l for l in alphs]\n","    return insert_list\n"],"metadata":{"id":"2FU_mXJV2I7F","executionInfo":{"status":"ok","timestamp":1726469436362,"user_tz":-330,"elapsed":452,"user":{"displayName":"HAMZA KRISHNAGIRI (RA2211004010570)","userId":"05325100219040306490"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["# Collecting all the words\n","# in a set(so that no word will repeat)\n","def colab_1(word, allow_switches=True):\n","    colab_1 = set()\n","    colab_1.update(DeleteLetter(word))\n","    if allow_switches:\n","        colab_1.update(Switch_(word))\n","    colab_1.update(Replace_(word))\n","    colab_1.update(insert_(word))\n","    return colab_1\n","\n","# collecting words using by allowing switches\n","def colab_2(word, allow_switches=True):\n","    colab_2 = set()\n","    edit_one = colab_1(word, allow_switches=allow_switches)\n","    for w in edit_one:\n","        if w:\n","            edit_two = colab_1(w, allow_switches=allow_switches)\n","            colab_2.update(edit_two)\n","    return colab_2\n"],"metadata":{"id":"AsKsvh4p2Njg","executionInfo":{"status":"ok","timestamp":1726469439239,"user_tz":-330,"elapsed":441,"user":{"displayName":"HAMZA KRISHNAGIRI (RA2211004010570)","userId":"05325100219040306490"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["# Only storing those values which are in the vocab\n","def get_corrections(word, probs, vocab, n=2):\n","    suggested_word = []\n","    best_suggestion = []\n","    suggested_word = list(\n","        (word in vocab and word) or colab_1(word).intersection(vocab)\n","        or colab_2(word).intersection(\n","            vocab))\n","\n","    # finding out the words with high frequencies\n","    best_suggestion = [[s, probs[s]] for s in list(reversed(suggested_word))]\n","    return best_suggestion\n"],"metadata":{"id":"lT-GLPjI2V_6","executionInfo":{"status":"ok","timestamp":1726469444335,"user_tz":-330,"elapsed":422,"user":{"displayName":"HAMZA KRISHNAGIRI (RA2211004010570)","userId":"05325100219040306490"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["eww"],"metadata":{"id":"1DRLNSzNnUlm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Input\n","my_word = input(\"Enter any word:\")\n","\n","# Counting word function\n","word_count = counting_words(main_set)\n","\n","# Function to calculate word probabilities\n","def probab_cal(word_count):\n","    total_words = sum(word_count.values())\n","    probs = {word: count/total_words for word, count in word_count.items()}\n","    return probs\n","\n","# Calculating probability\n","probs = probab_cal(word_count)\n","\n","# only storing correct words\n","tmp_corrections = get_corrections(my_word, probs, main_set, 2)\n","for i, word_prob in enumerate(tmp_corrections):\n","    if(i < 3):\n","        print(word_prob[0])\n","    else:\n","        break"],"metadata":{"id":"j_Q_-SMg2X2x","executionInfo":{"status":"ok","timestamp":1726469789077,"user_tz":-330,"elapsed":6936,"user":{"displayName":"HAMZA KRISHNAGIRI (RA2211004010570)","userId":"05325100219040306490"}},"outputId":"65d9ddf9-7a25-4c77-ed14-3d73862460c3","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":21,"outputs":[{"output_type":"stream","name":"stdout","text":["Enter any word:hui\n","hur\n","hue\n","hut\n"]}]}]}